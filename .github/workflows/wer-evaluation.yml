name: WER Evaluation

on:
  workflow_dispatch:
    inputs:
      force_all:
        description: 'Force run all models (ignore cache)'
        type: boolean
        default: false
#      setup_volume:
#        description: 'Setup Modal volume (only needed once)'
#        type: boolean
#        default: false
  push:
    paths:
      - 'openemr_whisper_wer/**'
      - '.github/workflows/wer-evaluation.yml'

jobs:
  # ============================================
  # Optional: Setup Modal persistent volume (run once)
  # ============================================
  setup-modal-volume:
    runs-on: ubuntu-latest
    #if: ${{ github.event.inputs.setup_volume == 'true' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Modal
        run: pip install modal

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Download dataset to Modal volume
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: modal run openemr_whisper_wer/kaggle_dataset.py

  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      whisper_changed: ${{ steps.filter.outputs.whisper }}
      canary_changed: ${{ steps.filter.outputs.canary }}
      granite_changed: ${{ steps.filter.outputs.granite }}
      phi4_changed: ${{ steps.filter.outputs.phi4 }}
      groq_changed: ${{ steps.filter.outputs.groq }}
      parakeet_changed: ${{ steps.filter.outputs.parakeet }}
      utils_changed: ${{ steps.filter.outputs.utils }}
      # Script hashes for cache keys
      whisper_hash: ${{ steps.hashes.outputs.whisper }}
      canary_hash: ${{ steps.hashes.outputs.canary }}
      granite_hash: ${{ steps.hashes.outputs.granite }}
      phi4_hash: ${{ steps.hashes.outputs.phi4 }}
      groq_hash: ${{ steps.hashes.outputs.groq }}
      parakeet_hash: ${{ steps.hashes.outputs.parakeet }}
      utils_hash: ${{ steps.hashes.outputs.utils }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Detect changed files
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            whisper:
              - 'openemr_whisper_wer/whisper_wer.py'
            canary:
              - 'openemr_whisper_wer/canary_wer.py'
            granite:
              - 'openemr_whisper_wer/granite_wer.py'
            phi4:
              - 'openemr_whisper_wer/phi4_wer.py'
            groq:
              - 'openemr_whisper_wer/wlv3t_on_groq.py'
            parakeet:
              - 'openemr_whisper_wer/parakeet_wer.py'
            utils:
              - 'openemr_whisper_wer/wer_utils.py'
              - 'openemr_whisper_wer/requirements.txt'

      - name: Calculate script hashes
        id: hashes
        run: |
          echo "whisper=${{ hashFiles('openemr_whisper_wer/whisper_wer.py') }}" >> $GITHUB_OUTPUT
          echo "canary=${{ hashFiles('openemr_whisper_wer/canary_wer.py') }}" >> $GITHUB_OUTPUT
          echo "granite=${{ hashFiles('openemr_whisper_wer/granite_wer.py') }}" >> $GITHUB_OUTPUT
          echo "phi4=${{ hashFiles('openemr_whisper_wer/phi4_wer.py') }}" >> $GITHUB_OUTPUT
          echo "groq=${{ hashFiles('openemr_whisper_wer/wlv3t_on_groq.py') }}" >> $GITHUB_OUTPUT
          echo "parakeet=${{ hashFiles('openemr_whisper_wer/parakeet_wer.py') }}" >> $GITHUB_OUTPUT
          echo "utils=${{ hashFiles('openemr_whisper_wer/wer_utils.py', 'openemr_whisper_wer/requirements.txt') }}" >> $GITHUB_OUTPUT

  # ============================================
  # Model evaluation jobs (run in parallel)
  # ============================================

  evaluate-whisper-turbo:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-whisper-large-v3-turbo.csv
          key: wer-whisper-turbo-${{ needs.detect-changes.outputs.whisper_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
            echo "Force run requested"
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
            echo "Using cached results"
          else
            echo "ran=true" >> $GITHUB_OUTPUT
            echo "Running evaluation"
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python whisper_wer.py --output results-whisper-large-v3-turbo.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-whisper-large-v3-turbo
          path: openemr_whisper_wer/results-whisper-large-v3-turbo.csv
          retention-days: 90

  evaluate-whisper-v3:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-whisper-large-v3.csv
          key: wer-whisper-v3-${{ needs.detect-changes.outputs.whisper_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python whisper_wer.py --output results-whisper-large-v3.csv --use-large-v3

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-whisper-large-v3
          path: openemr_whisper_wer/results-whisper-large-v3.csv
          retention-days: 90

  evaluate-canary:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-canary-1b-v2.csv
          key: wer-canary-${{ needs.detect-changes.outputs.canary_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python canary_wer.py --output results-canary-1b-v2.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-canary-1b-v2
          path: openemr_whisper_wer/results-canary-1b-v2.csv
          retention-days: 90

  evaluate-granite:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-granite-speech-3.3-8b.csv
          key: wer-granite-${{ needs.detect-changes.outputs.granite_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python granite_wer.py --output results-granite-speech-3.3-8b.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-granite-speech-3.3-8b
          path: openemr_whisper_wer/results-granite-speech-3.3-8b.csv
          retention-days: 90

  evaluate-phi4:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-phi4-multimodal.csv
          key: wer-phi4-${{ needs.detect-changes.outputs.phi4_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python phi4_wer.py --output results-phi4-multimodal.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-phi4-multimodal
          path: openemr_whisper_wer/results-phi4-multimodal.csv
          retention-days: 90

  evaluate-groq:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-groq-whisper-large-v3-turbo.csv
          key: wer-groq-${{ needs.detect-changes.outputs.groq_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          cd openemr_whisper_wer
          python wlv3t_on_groq.py --output results-groq-whisper-large-v3-turbo.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-groq-whisper-large-v3-turbo
          path: openemr_whisper_wer/results-groq-whisper-large-v3-turbo.csv
          retention-days: 90

  evaluate-parakeet:
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      ran: ${{ steps.check.outputs.ran }}
    steps:
      - uses: actions/checkout@v4

      - name: Check cache for previous results
        id: cache
        uses: actions/cache@v4
        with:
          path: openemr_whisper_wer/results-parakeet-tdt-1.1b.csv
          key: wer-parakeet-${{ needs.detect-changes.outputs.parakeet_hash }}-${{ needs.detect-changes.outputs.utils_hash }}

      - name: Determine if evaluation needed
        id: check
        run: |
          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            echo "ran=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.cache.outputs.cache-hit }}" == "true" ]; then
            echo "ran=false" >> $GITHUB_OUTPUT
          else
            echo "ran=true" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        if: steps.check.outputs.ran == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        if: steps.check.outputs.ran == 'true'
        run: pip install -r openemr_whisper_wer/requirements.txt

      - name: Run WER evaluation
        if: steps.check.outputs.ran == 'true'
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          cd openemr_whisper_wer
          python parakeet_wer.py --output results-parakeet-tdt-1.1b.csv

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-parakeet-tdt-1.1b
          path: openemr_whisper_wer/results-parakeet-tdt-1.1b.csv
          retention-days: 90

  # ============================================
  # Comparison table generation
  # ============================================

  generate-comparison:
    needs:
      - detect-changes
      - evaluate-whisper-turbo
      - evaluate-whisper-v3
      - evaluate-canary
      - evaluate-granite
      - evaluate-phi4
      - evaluate-groq
      - evaluate-parakeet
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install pandas
        run: pip install pandas

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
          pattern: results-*
          merge-multiple: false

      - name: Generate comparison table
        run: |
          python << 'EOF'
          import pandas as pd
          import os
          from datetime import datetime

          models = [
              ('whisper-large-v3-turbo', 'results-whisper-large-v3-turbo'),
              ('whisper-large-v3', 'results-whisper-large-v3'),
              ('canary-1b-v2', 'results-canary-1b-v2'),
              ('granite-speech-3.3-8b', 'results-granite-speech-3.3-8b'),
              ('phi4-multimodal', 'results-phi4-multimodal'),
              ('groq-whisper-large-v3-turbo', 'results-groq-whisper-large-v3-turbo'),
              ('parakeet-tdt-1.1b', 'results-parakeet-tdt-1.1b'),
          ]

          results = []
          all_samples = {}

          for model_name, artifact_name in models:
              # Try to find the results file (artifact downloads to subdir with artifact name)
              csv_path = f"artifacts/{artifact_name}/{artifact_name}.csv"

              if os.path.exists(csv_path):
                  try:
                      df = pd.read_csv(csv_path)
                      # Calculate stats - handle missing error column
                      if 'error' in df.columns:
                          valid = df[df['error'].isna() | (df['error'] == '')]
                      else:
                          valid = df
                      avg_wer = valid['wer'].mean() if len(valid) > 0 else None
                      samples = len(valid)
                      total = len(df)

                      results.append({
                          'model': model_name,
                          'avg_wer': avg_wer,
                          'samples': samples,
                          'total': total,
                          'status': 'available'
                      })

                      # Store per-sample results for detailed comparison
                      for _, row in df.iterrows():
                          name = row['name']
                          if name not in all_samples:
                              all_samples[name] = {}
                          wer = row.get('wer')
                          has_error = 'error' in row and pd.notna(row.get('error')) and row.get('error')
                          if pd.notna(wer) and not has_error:
                              all_samples[name][model_name] = wer
                  except Exception as e:
                      results.append({
                          'model': model_name,
                          'avg_wer': None,
                          'samples': 0,
                          'total': 0,
                          'status': f'error: {str(e)[:50]}'
                      })
              else:
                  results.append({
                      'model': model_name,
                      'avg_wer': None,
                      'samples': 0,
                      'total': 0,
                      'status': 'no results'
                  })

          # Generate markdown summary
          now = datetime.now().strftime("%Y-%m-%d %H:%M UTC")
          commit = os.environ.get('GITHUB_SHA', 'unknown')[:7]

          lines = []
          lines.append("## WER Model Comparison Results\n")
          lines.append(f"**Date:** {now}")
          lines.append(f"**Commit:** `{commit}`\n")
          lines.append("### Summary Table\n")
          lines.append("| Model | Avg WER | Samples | Status |")
          lines.append("|-------|---------|---------|--------|")

          # Sort by avg_wer (best first)
          sorted_results = sorted(results, key=lambda x: x['avg_wer'] if x['avg_wer'] is not None else 999)

          for r in sorted_results:
              if r['avg_wer'] is not None:
                  wer_str = f"{r['avg_wer']:.2%}"
                  if r['avg_wer'] < 0.10:
                      status_icon = "ðŸ¥‡"
                  elif r['avg_wer'] < 0.15:
                      status_icon = "ðŸ¥ˆ"
                  elif r['avg_wer'] < 0.20:
                      status_icon = "ðŸ¥‰"
                  elif r['avg_wer'] < 0.30:
                      status_icon = "ðŸŸ¡"
                  else:
                      status_icon = "ðŸ”´"
              else:
                  wer_str = "N/A"
                  status_icon = "âšª"

              lines.append(f"| {r['model']} | {wer_str} | {r['samples']}/{r['total']} | {status_icon} {r['status']} |")

          # Add per-sample comparison if we have data
          if all_samples:
              lines.append("\n### Per-Sample Comparison\n")

              # Get models that have results
              active_models = [r['model'] for r in results if r['avg_wer'] is not None]

              if active_models:
                  header = "| Sample |"
                  divider = "|--------|"
                  for m in active_models:
                      short_name = m.replace('whisper-large-v3', 'wlv3').replace('-turbo', '-t').replace('multimodal', 'mm')
                      header += f" {short_name} |"
                      divider += "--------|"

                  lines.append(header + " Best |")
                  lines.append(divider + "------|")

                  for sample_name, wers in sorted(all_samples.items()):
                      if not wers:
                          continue
                      row = f"| {sample_name[:20]} |"
                      best_wer = min(wers.values()) if wers else None
                      for m in active_models:
                          if m in wers:
                              wer = wers[m]
                              marker = " **" if wer == best_wer else ""
                              end_marker = "**" if wer == best_wer else ""
                              row += f" {marker}{wer:.0%}{end_marker} |"
                          else:
                              row += " - |"
                      # Add best model name
                      if best_wer is not None:
                          best_model = min(wers.items(), key=lambda x: x[1])[0]
                          best_short = best_model.replace('whisper-large-v3', 'wlv3').replace('-turbo', '-t').replace('multimodal', 'mm')
                          row += f" {best_short} |"
                      else:
                          row += " - |"
                      lines.append(row)

          lines.append("\n---\n")
          lines.append("### Legend")
          lines.append("- ðŸ¥‡ Excellent (WER < 10%)")
          lines.append("- ðŸ¥ˆ Very Good (WER < 15%)")
          lines.append("- ðŸ¥‰ Good (WER < 20%)")
          lines.append("- ðŸŸ¡ Fair (WER < 30%)")
          lines.append("- ðŸ”´ Poor (WER >= 30%)")
          lines.append("- âšª No results available")
          lines.append("\n*Results cached based on script content hash. Use 'Force run all models' to regenerate.*")

          summary = "\n".join(lines)

          # Write to GitHub step summary
          with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/stdout'), 'a') as f:
              f.write(summary)

          print(summary)

          # Save comparison CSV
          comparison_df = pd.DataFrame(results)
          comparison_df.to_csv('comparison_results.csv', index=False)
          EOF

      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        with:
          name: wer-comparison
          path: comparison_results.csv
          retention-days: 90