name: RAG Medical Summarization (All Patients)

on:
  workflow_dispatch:
    inputs:
      run_llama_3_1_8b:
        description: 'Run Llama 3.1 8B model'
        type: boolean
        default: true
      run_medgemma_4b:
        description: 'Run MedGemma 4B-IT model'
        type: boolean
        default: true
      run_medgemma_27b_4bit:
        description: 'Run MedGemma 27B-Text-IT (4-bit) model'
        type: boolean
        default: true
      run_gpt_oss_20b:
        description: 'Run Groq GPT-OSS-20B model'
        type: boolean
        default: true
      run_gpt_oss_120b:
        description: 'Run Groq GPT-OSS-120B model'
        type: boolean
        default: true
      run_qwen3_32b:
        description: 'Run Groq Qwen3-32B model'
        type: boolean
        default: true
      run_llama4_scout:
        description: 'Run Groq Llama-4-Scout-17B model'
        type: boolean
        default: true
  push:
    paths:
      - 'rag_models/RAG_To_See_MedGemma_Performance/**'
      - '.github/workflows/rag-summarization.yml'

env:
  PYTHON_VERSION: '3.11'
  WORK_DIR: 'rag_models/RAG_To_See_MedGemma_Performance'

jobs:
  # ============================================
  # JOB 0: Deploy Both Evaluator Services
  # Must run BEFORE all model jobs
  # ============================================
  deploy-evaluators:
    name: üöÄ Deploy Evaluator Services
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Modal
        run: pip install modal

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Deploy Text Evaluator Service
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Deploying text evaluator service (BLEU, ROUGE, SBERT, BERTScore)..."
          modal deploy text_evaluator_service.py
          echo "‚úÖ Text evaluator service deployed successfully"

      - name: Deploy Entity Evaluator Service
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Deploying entity evaluator service (scispaCy, MedCAT)..."
          modal deploy entity_evaluator_service.py
          echo "‚úÖ Entity evaluator service deployed successfully"

      - name: Warm up Text Evaluator
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üî• Warming up text evaluator..."
          modal run text_evaluator_service.py
          echo "‚úÖ Text evaluator ready"

      - name: Warm up Entity Evaluator
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üî• Warming up entity evaluator (this may take 5-10 min)..."
          modal run entity_evaluator_service.py
          echo "‚úÖ Entity evaluator ready"

  # ============================================
  # JOB 1: Llama 3.1 8B RAG Summarization
  # File: llama_3_1_8b_modal.py
  # ============================================
  llama-3-1-8b:
    name: ü¶ô Llama 3.1 8B (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_llama_3_1_8b != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run Llama 3.1 8B Pipeline
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running Llama 3.1 8B summarization for ALL patients from Notion"
          modal run llama_3_1_8b_modal.py --output-dir results/llama-3.1-8b

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/llama-3.1-8b"
          
          echo "## ü¶ô Llama 3.1 8B Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_llama-3.1-8b.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_llama-3.1-8b.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: llama-3.1-8b-results
          path: ${{ env.WORK_DIR }}/results/llama-3.1-8b/
          retention-days: 90

  # ============================================
  # JOB 2: MedGemma 4B-IT RAG Summarization
  # File: main_medgemma_4b_modal.py
  # ============================================
  medgemma-4b:
    name: üè• MedGemma 4B-IT (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_medgemma_4b != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run MedGemma 4B-IT Pipeline
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running MedGemma 4B-IT summarization for ALL patients from Notion"
          modal run main_medgemma_4b_modal.py --output-dir results/medgemma-4b

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/medgemma-4b"
          
          echo "## üè• MedGemma 4B-IT Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_medgemma-4b.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_medgemma-4b.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: medgemma-4b-results
          path: ${{ env.WORK_DIR }}/results/medgemma-4b/
          retention-days: 90

  # ============================================
  # JOB 3: MedGemma 27B-Text-IT (4-bit) RAG Summarization
  # File: medgemma_27b_4bit_modal.py
  # ============================================
  medgemma-27b-4bit:
    name: üè• MedGemma 27B (4-bit) (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_medgemma_27b_4bit != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run MedGemma 27B (4-bit) Pipeline
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running MedGemma 27B (4-bit) summarization for ALL patients from Notion"
          modal run medgemma_27b_4bit_modal.py --output-dir results/medgemma-27b-4bit

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/medgemma-27b-4bit"
          
          echo "## üè• MedGemma 27B (4-bit) Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_medgemma-27b-4bit.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_medgemma-27b-4bit.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: medgemma-27b-4bit-results
          path: ${{ env.WORK_DIR }}/results/medgemma-27b-4bit/
          retention-days: 90

  # ============================================
  # JOB 4: Groq GPT-OSS-20B RAG Summarization
  # File: gpt_oss_20b_modal.py
  # ============================================
  gpt-oss-20b:
    name: üöÄ Groq GPT-OSS-20B (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_gpt_oss_20b != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run Groq GPT-OSS-20B Pipeline
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running Groq GPT-OSS-20B summarization for ALL patients from Notion"
          modal run gpt_oss_20b_modal.py --output-dir results/gpt-oss-20b

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/gpt-oss-20b"
          
          echo "## üöÄ Groq GPT-OSS-20B Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_gpt-oss-20b.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_gpt-oss-20b.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: gpt-oss-20b-results
          path: ${{ env.WORK_DIR }}/results/gpt-oss-20b/
          retention-days: 90

  # ============================================
  # JOB 5: Groq GPT-OSS-120B RAG Summarization
  # File: gpt_120b_modal.py
  # ============================================
  gpt-oss-120b:
    name: üöÄ Groq GPT-OSS-120B (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_gpt_oss_120b != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run Groq GPT-OSS-120B Pipeline
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running Groq GPT-OSS-120B summarization for ALL patients from Notion"
          modal run gpt_120b_modal.py --output-dir results/gpt-oss-120b

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/gpt-oss-120b"
          
          echo "## üöÄ Groq GPT-OSS-120B Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_gpt-oss-120b.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_gpt-oss-120b.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: gpt-oss-120b-results
          path: ${{ env.WORK_DIR }}/results/gpt-oss-120b/
          retention-days: 90

  # ============================================
  # JOB 6: Groq Qwen3-32B RAG Summarization
  # File: qwen_32b_modal.py
  # ============================================
  qwen3-32b:
    name: üß† Groq Qwen3-32B (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_qwen3_32b != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run Groq Qwen3-32B Pipeline
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running Groq Qwen3-32B summarization for ALL patients from Notion"
          modal run qwen_32b_modal.py --output-dir results/qwen3-32b

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/qwen3-32b"
          
          echo "## üß† Groq Qwen3-32B Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_qwen3-32b.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_qwen3-32b.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: qwen3-32b-results
          path: ${{ env.WORK_DIR }}/results/qwen3-32b/
          retention-days: 90

  # ============================================
  # JOB 7: Groq Llama-4-Scout-17B RAG Summarization
  # File: groq_llama_4_scout_modal.py
  # ============================================
  llama4-scout:
    name: ü¶ô Groq Llama-4-Scout-17B (All Patients)
    needs: deploy-evaluators
    if: github.event.inputs.run_llama4_scout != 'false'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install modal notion-client httpx pandas python-dotenv

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Run Groq Llama-4-Scout-17B Pipeline
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          VISHNU_NOTION: ${{ secrets.VISHNU_NOTION }}
          VISHNU_NOTION_DB_ID: ${{ secrets.VISHNU_NOTION_DB_ID }}
        run: |
          cd ${{ env.WORK_DIR }}
          echo "üöÄ Running Groq Llama-4-Scout-17B summarization for ALL patients from Notion"
          modal run groq_llama_4_scout_modal.py --output-dir results/llama4-scout

      - name: Display Results Summary
        run: |
          RESULTS_DIR="${{ env.WORK_DIR }}/results/llama4-scout"
          
          echo "## ü¶ô Groq Llama-4-Scout-17B Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "$RESULTS_DIR/evaluation_results_llama4-scout.csv" ]; then
            echo "### Evaluation Metrics (including Entity Recall)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat "$RESULTS_DIR/evaluation_results_llama4-scout.csv" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Results CSV not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: llama4-scout-results
          path: ${{ env.WORK_DIR }}/results/llama4-scout/
          retention-days: 90

  # ============================================
  # JOB 8: Compare All Model Results
  # Generates clear comparison table with averages
  # ============================================
  compare-results:
    name: üìä Compare All Models
    needs: [llama-3-1-8b, medgemma-4b, medgemma-27b-4bit, gpt-oss-20b, gpt-oss-120b, qwen3-32b, llama4-scout]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pandas tabulate

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-results

      - name: Generate Consolidated Report with Averages
        run: |
          python << 'EOF'
          import pandas as pd
          from pathlib import Path
          from tabulate import tabulate

          results_dir = Path("all-results")
          all_data = []

          # Load all CSV files
          for csv_file in results_dir.rglob("evaluation_results_*.csv"):
              print(f"üìÇ Loading: {csv_file}")
              df = pd.read_csv(csv_file)
              all_data.append(df)

          if not all_data:
              print("‚ùå No CSV files found!")
              exit(1)

          # Combine all data
          combined = pd.concat(all_data, ignore_index=True)

          # Remove existing AVERAGE rows
          combined = combined[combined['patient_name'] != 'AVERAGE']

          print(f"‚úÖ Loaded {len(combined)} patient records")
          print(f"   Models: {combined['model'].unique().tolist()}")

          # Define metrics
          metrics = ['bleu', 'rouge_l', 'sbert_coherence', 'bert_f1', 
                     'scispacy_entity_recall', 'medcat_entity_recall', 
                     'total_time_s']
          available_metrics = [m for m in metrics if m in combined.columns]

          # Calculate averages per model
          comparison = combined.groupby('model')[available_metrics].mean().reset_index()
          comparison = comparison.sort_values('bert_f1', ascending=False).reset_index(drop=True)

          # Add rank column
          comparison.insert(0, 'Rank', range(1, len(comparison) + 1))

          # Rename columns for display
          comparison.columns = ['Rank', 'Model', 'BLEU', 'ROUGE-L', 'SBERT', 'BERTScore', 
                                'scispaCy Recall', 'MedCAT Recall', 'Time (s)']

          # Round values
          for col in comparison.columns[2:]:
              if col == 'Time (s)':
                  comparison[col] = comparison[col].round(2)
              else:
                  comparison[col] = comparison[col].round(4)

          # Save comparison CSV
          comparison.to_csv(results_dir / "model_comparison.csv", index=False)

          # Print table to console
          print("\n" + "=" * 120)
          print("üìä MODEL COMPARISON - AVERAGE METRICS ACROSS ALL PATIENTS")
          print("=" * 120)
          print(tabulate(comparison, headers='keys', tablefmt='grid', showindex=False))
          print("=" * 120)

          # Generate HTML table for GitHub Summary
          html = """<h2>üìä Model Comparison - Average Metrics Across All Patients</h2>

          <table>
          <tr style="background-color: #4CAF50; color: white;">
              <th>Rank</th>
              <th>Model</th>
              <th>BLEU</th>
              <th>ROUGE-L</th>
              <th>SBERT</th>
              <th>BERTScore</th>
              <th>scispaCy Recall</th>
              <th>MedCAT Recall</th>
              <th>Time (s)</th>
          </tr>
          """

          for idx, row in comparison.iterrows():
              bg_color = "#f2f2f2" if idx % 2 == 0 else "white"
              # Highlight best model (rank 1)
              if row['Rank'] == 1:
                  bg_color = "#c8e6c9"
          
              html += f"""<tr style="background-color: {bg_color};">
                  <td><b>{int(row['Rank'])}</b></td>
                  <td><b>{row['Model']}</b></td>
                  <td>{row['BLEU']:.4f}</td>
                  <td>{row['ROUGE-L']:.4f}</td>
                  <td>{row['SBERT']:.4f}</td>
                  <td>{row['BERTScore']:.4f}</td>
                  <td>{row['scispaCy Recall']:.4f}</td>
                  <td>{row['MedCAT Recall']:.4f}</td>
                  <td>{row['Time (s)']:.2f}</td>
              </tr>
              """

          html += "</table>"

          # Add best models section
          html += """<h2>üèÜ Best Models by Metric</h2>
          <table>
          <tr style="background-color: #2196F3; color: white;">
              <th>Metric</th>
              <th>Best Model</th>
              <th>Score</th>
          </tr>
          """

          # Reset column names for finding best
          comparison.columns = ['Rank', 'Model', 'BLEU', 'ROUGE-L', 'SBERT', 'BERTScore', 
                                'scispaCy Recall', 'MedCAT Recall', 'Time (s)']

          rankings = [
              ('BERTScore', True),
              ('BLEU', True),
              ('ROUGE-L', True),
              ('SBERT', True),
              ('scispaCy Recall', True),
              ('MedCAT Recall', True),
              ('Time (s)', False),  # Lower is better
          ]

          for idx, (metric, higher_better) in enumerate(rankings):
              if higher_better:
                  best_idx = comparison[metric].idxmax()
              else:
                  best_idx = comparison[metric].idxmin()
          
              best_model = comparison.loc[best_idx, 'Model']
              best_value = comparison.loc[best_idx, metric]
          
              bg_color = "#f2f2f2" if idx % 2 == 0 else "white"
          
              if metric == 'Time (s)':
                  html += f"""<tr style="background-color: {bg_color};">
                      <td>{metric} (lower is better)</td>
                      <td><b>{best_model}</b></td>
                      <td>{best_value:.2f}s</td>
                  </tr>
                  """
              else:
                  html += f"""<tr style="background-color: {bg_color};">
                      <td>{metric}</td>
                      <td><b>{best_model}</b></td>
                      <td>{best_value:.4f}</td>
                  </tr>
                  """

          html += "</table>"

          # Save HTML report
          with open(results_dir / "comparison_report.html", 'w') as f:
              f.write(html)

          # Also save as markdown for artifact
          with open(results_dir / "comparison_report.md", 'w') as f:
              f.write(tabulate(comparison, headers='keys', tablefmt='pipe', showindex=False))

          print("\n‚úÖ Reports generated successfully!")
          EOF

      - name: Append to GitHub Summary
        run: |
          if [ -f "all-results/comparison_report.html" ]; then
            cat all-results/comparison_report.html >> $GITHUB_STEP_SUMMARY
          else
            echo "<h2>‚ö†Ô∏è No comparison report generated</h2>" >> $GITHUB_STEP_SUMMARY
            echo "<p>Some model jobs may have failed or been skipped.</p>" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Consolidated Results
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-comparison
          path: |
            all-results/model_comparison.csv
            all-results/comparison_report.html
            all-results/comparison_report.md
          retention-days: 90

  # ============================================
  # JOB 9: Stop Both Evaluator Services (Cleanup)
  # Runs after all model jobs complete
  # ============================================
  cleanup-evaluators:
    name: üßπ Stop Evaluator Services
    needs: [llama-3-1-8b, medgemma-4b, medgemma-27b-4bit, gpt-oss-20b, gpt-oss-120b, qwen3-32b, llama4-scout]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Modal
        run: pip install modal

      - name: Authenticate Modal
        env:
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET

      - name: Stop Text Evaluator Service
        run: |
          echo "üßπ Stopping text evaluator service..."
          modal app stop text-evaluator-service || echo "‚ö†Ô∏è Service may already be stopped or not exist"

      - name: Stop Entity Evaluator Service
        run: |
          echo "üßπ Stopping entity evaluator service..."
          modal app stop entity-evaluator-service || echo "‚ö†Ô∏è Service may already be stopped or not exist"
          echo "‚úÖ Cleanup complete"